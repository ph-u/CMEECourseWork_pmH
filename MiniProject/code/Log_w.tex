
% Author: PokMan Ho pok.ho19@imperial.ac.uk
% Script: LogisticTmp.tex
% Desc: `LaTex` report framework -- Logistic Growth
% Input: none
% Output: none
% Arguments: 0
% Date: Oct 2019

\documentclass[a4paper, 11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref, setspace, lineno, amsmath, amssymb, longtable}
\usepackage{float} %% fix graphs at where they should be

%% test insert graphs
\usepackage{graphicx}
\graphicspath{ {../results/} } %% <https://www.overleaf.com/learn/latex/Inserting_Images>

%% test insert variables
\newcommand{\pml}{phenological model}
\newcommand{\pms}{phenological models}
\newcommand{\Pml}{Phenological model}
\newcommand{\Pms}{Phenological models}
\newcommand{\ReportTitle}{Microbial growth models -- which is better and why?} %% <https://stackoverflow.com/questions/1211888/is-there-any-way-i-can-define-a-variable-in-latex>
\newcommand{\ReportAuthor}{PokMan HO}
\newcommand{\ReportAffil}{Department of Life Sciences, Faculty of Natural Sciences,\\Imperial College London}
\newcommand{\Disclaim}{\textbf{A Mini-project submitted in partial fulfilment of the requirements for the degree of Master of Research at Imperial College London\\\\Formatted in the journal style of the \textit{Nature} Journal\\Submitted for the MRes in Computational Methods of Ecology and Evolution}}
\newcommand{\fve}{Verhulst (classical)}
\newcommand{\fgo}{modified Gompertz}
\newcommand{\fba}{Baranyi}
\newcommand{\fbu}{Buchanan}
\newcommand{\fqu}{quadratic}
\newcommand{\fcu}{cubic}
\newcommand{\pop}{population size}
\newcommand{\pps}{population sizes}

\title{\ReportTitle}
\author{\ReportAuthor\ (CID: 01786076)}
\date{}

%% citation
\usepackage[%
autocite    = superscript,
backend     = bibtex,
sortcites   = true,
style       = nature
]{biblatex}
\bibliography{../reference/Log_r.bib} %% <https://tex.stackexchange.com/questions/6805/bib-library-file-in-a-different-directory-how-to-use-mendeley-centralised-b>

%% set as required
\doublespacing
\linenumbers

\begin{document}
	\begin{center}
		\Huge\textbf{\ReportTitle}\\
		\LARGE\ReportAuthor\\
		\Large\ReportAffil
	\end{center}
	\begin{figure}[h]
		\centering\includegraphics[width=\linewidth]{icl.jpg}
	\end{figure}
	\begin{flushright}
		\Large Approximate Word Count: %% insert approx word count
	\end{flushright}
	\clearpage
	
	\maketitle
	\section*{Abstract}
	
	
	\section*{Introduction}
	\Pms\ are expected to fit data trends within its biological field.  Yet due to different reasons, models developed and published from one sample may not fit the others.  These reasons can be data variabilities, confounding factors, inaccurate assumptions or models being too-specific.  This data-mining project is aimed at comparing and contrasting published \pms\ on microbial \pop\ data, highlighting which is a better model under what conditions.  The hypotheses are:
	\begin{itemize}
		\item published \pms\ are significantly better fits than polynomials for describing microbial growths;
		\item appropriate \pml(s) is/are identifiable through distinguishable microbial \pop\ parameter values; and
		\item parameter values of each \pml\ from successful fits are clusters with well-defined boundaries between models.
	\end{itemize}
	
	\section*{Methods}
	
	Microbial \pps\ data were given, sourced from ten different publications (Table\ref{table:source}).  The collection contained different microbial clades growing at various conditions for varying times.  Also, these data were recorded in multiple time and population units.  Some of the population data were direct counts while some were not.
	
	Experimental microbial population growth data library were divided into individual data subsets through six filters (``Temperature (in $^o$C)", ``Microbial clade", ``growth substrate materials", ``experimental replicate number", ``population data recording unit" and ``data source").  Records with data unit ``OD\_595" were scaled into optical density percentages (i.e. data*100) to facilitate general analyses workflow.  Independent (or explanatory) variable was ``Time (hr)" and dependent (or response) variable was ``\pop".
	
	Some raw data were recorded in minutes (instead of hour).  This record artifact was not corrected because of two reasons: 1. shape of curves were the main concern instead of independent variable's scale; and 2. the unit was consistent within each data subset.
	
	\subsection*{Model assessment}
	Six candidate models were assessed, four phenological and two polynomial equations.  They were ``\fve"\autocite{mckendrick1912xlv}, ``\fgo"\autocite{GIL200689}, ``\fba"\autocite{baranyi1993modeling}, ``\fbu"\autocite{buchanan1993differentiation}, ``\fqu" and ``\fcu". Non-linear least square (NLLS) method was used only on the four \pms\ and linear approach was taken on the two polynomials.  Starting values selection (for \pms\ only) was described below:
	
	Initial (N0) and final (K) \pps\ were selected to be the minimum and maximum values of each data subset respectively.  Maximum growth rate (r.max) and relative time lag (t.lag) were obtained through recursive linear modelling with shrinking independent range (from maximum and minimum).  In this project 5\% was chosen as the shrinking threshold assuming this resolution was sufficient for initiating NLLS fits.  The final slope value (i.e. r.max) would be a positive finite number with the highest R$^{2}$ value (i.e. best-fit slope on considered data).  The x-intercept for that slope was the t.lag.  Time which this r.max linear model intersected with K was regarded as the time achieving carrying capacity (t.K).  Population data was then classified into three groups (gx) according to the time: g1 $\leq$ t.lag $<$ g2 $<$ t.K $\leq$ g3.  Inputs for \pms were listed below (popn \& time were the dependent and independent variables respectively):
	
	\begin{tabular}{rl}
		\fve: & popn = $f$(N0, K, r.max, time)\\
		\fgo: & popn = $f$(N0, K, r.max, time, t.lag)\\
		\fba: & popn = $f$(N0, K, r.max, time, t.lag)\\
		\fbu: & popn = $f$(N0, K, r.max, time, t.lag, gx)
	\end{tabular}
	
	All test starting values were than sampled from normal distribution with mean as the estimated value and standard deviation (sd) of 1.  The sd value was chosen because of different reasons for each parameters.  N0 and K were directly extracted from the raw experimental data.  Hence a small sd was precise assuming this extraction was an accurate estimate. r.max was a guesstimated value.  So a large sd would be sufficient for getting the ``true" value with fair accuracy.  100 trials were done as a optimal value under a trade-off between efficiency and accuracy.
	
	AIC\autocite{johnson2004model,akaike1998information,burnhamdr} was used to select optimal parameter values (i.e. meeting the ``minimal AIC +2"\autocite{burnham2004multimodel} criteria) within each \pml\ and best candidate model(s) for a data subset.  All available parameter sets were included in principal component analysis (PCA).  AIC tolerance threshold was expanded to min(AIC)+2\autocite{burnham2004multimodel} to incorporate more accepted models for analyses.
	
	\subsection*{Statistical analysis}
	Kruskal test was used for identify the best-fit model among all included model because the count was categorical and not assumed being normally-distributed.  Pairwise Nemenyi comparisons would be carried out to identify the best test if p-value of the test was significant.
	
	Parameter weights were assessed across \pms\ by PCA R-way analysis method on natural-logged parameter data.  Datasets would be expected clustering together if parameter(s) were representing the observed data.  Then with Kruskal-Wallis test, each parameter was tested for statistical differences across \pms.  Post-hoc Tukey pairwise comparisons would be carried out upon significance.
	
	\subsection*{Main Assumptions}
	\begin{itemize}
		\item there was no negative population growth throughout experiments from source publications.  Data not fitted this assumption were set to zeros;
		\item all parameter estimates converged to global optimal using NLLS method.
	\end{itemize}
	
	\subsection*{Computing tools}
	R (ver 3.6.0)\autocite{Rcore} was used with packages ``minpack.lm"\autocite{minpacklm} (NLLS), ``stats"\autocite{Rcore} (Kruskal test and PCA) and ``PMCMR"\autocite{PMCMR} (post-hoc Nemenyi pairwise comparisons).  Python (ver 3.7.3)\autocite{py3} was used with package ``subprocess"\autocite{py3} (streamline project workflow).
	
	\section*{Results}
	\begin{figure}[H]
		\centering
		\includegraphics[width=.8\linewidth]{../results/barplot_BestModel.pdf}
		\caption{Barplot showing the number of ``best model" identification under AIC model-selection methods with ``
			%%insert_num_here
			" statistic $X^{2}$ = 
			%% insert_num_here
			, df = 
			%% insert_num_here
			, p = 
			%% insert_num_here
		}\label{barPT}
	\end{figure}
	From Fig.\ref{barPT}, large fluctuations between each model to be described as ``best-fit" were observed.  However the occurrence difference was not statistical significant.  Among the counts, there were 
	%% insert_num_here
	datasets with more than one ``best-fit" models.  \fve\ and \fcu\ were the top two models selected as ``best-fit" for the 
	%% insert_num_here
	 datasets (
	 %%insert_num_here
	  for \fve\ and 
	 %% insert_num_here
	  for \fcu
	 ).  There are 
	 %% insert_num_here
	  datasets calling both ``best-fit" at the same trial.  Between \fba\ and \fqu, the counts were 
	 %%insert_num_here
	  and 
	 %%insert_num_here
	  respectively with 
	 %% insert_num_here
	 datasets calling both models ``best-fit".  The only outstanding performance was from \fgo, which 
	 %% insert_num_here
	  datasets were called it as ``best-fit".
	  
	 \begin{figure}[H]
	 	\centering
	 	\includegraphics[width=\linewidth]{../results/Log_PCA.pdf}
	 	\caption{Biplot of Principal Component Analysis (PCA) comparing \pms\ using estimated parameter values with ``minimal AIC +2"\autocite{burnham2004multimodel} evaluations.}\label{biptPCA}
	 \end{figure}
 \begin{figure}[H]
 	\centering
 	\includegraphics[width=\linewidth]{../results/Log_PCA_kt.pdf}
 	\caption{P-value summary between models on the four parameters under post-hoc Tukey-Dist pairwise comparison from Kruskal-Wallis Test.  Kruskal tests for all four factors were significant (
 		%% insert_num_here
 		: $X^{2}$ =
 		%% insert_num_here
 		, df = 
 		%% insert_num_here
 		, p-value = 
 		%% insert_num_here
 		; 
 		%% insert_num_here
 		: $X^{2}$ =
 		%% insert_num_here
 		, df = 
 		%% insert_num_here
 		, p-value = 
 		%% insert_num_here
 		; 
 		%% insert_num_here
 		: $X^{2}$ =
 		%% insert_num_here
 		, df = 
 		%% insert_num_here
 		, p-value = 
 		%% insert_num_here
 		; 
 		%% insert_num_here
 		: $X^{2}$ =
 		%% insert_num_here
 		, df = 
 		%% insert_num_here
 		, p-value = 
 		%% insert_num_here
 		).}\label{pValDraw}
 \end{figure}
 \begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{../results/Log_boxPerFac.pdf}
	\caption{Boxplot of log parameter values grouped by \pms.  Statistical results were summarized in Fig.\ref{pValDraw}}\label{boxFac}
\end{figure}
 In Fig.\ref{biptPCA}, principal component 1 (PC1) was capturing 
 %% insert_num_here
 \% variability.  It was composed approximately by 
 %% insert_num_here
  N0, 
 %%insert_num_here
  K, 
 %% insert_num_here
  r.max and 
 %% insert_num_here
  t.lag.  PC2 was capturing 
  %%insert_num_here
  \% variability.  It was composed approximately by 
 %% insert_num_here
  N0, 
 %%insert_num_here
  K, 
 %% insert_num_here
  r.max and 
 %% insert_num_here
  t.lag.  There were 
 %% insert_num_here
  datasets with \pms\ fitting, although they may not be the ``best-fit" ones.  Datasets 
 %% insert_num_here
  were strictly limited to polynomial-fitting (Fig.\ref{lineOut}).  \fve\ was having the widest neutral coverage across parameter space (Fig.\ref{biptPCA},\ref{boxFac}).  All other three models (\fgo, \fba\ and \fbu) were generally modelling within the \fve\ coverage (Fig.\ref{boxFac}).  \fgo\ was the second widest coverage model but \fve\ was evaluated better if both equations fitted the same dataset (Fig.\ref{barPT}).  More successful trials were towards positive responses for N0, K and r.max.  \fba\ was a specific model more specified in describing datasets with negative responses towards most parameter factors (all except r.max).  \fba\ had the strictest r.max acceptance for successful NLLS modelling (Fig.\ref{boxFac}).  \fbu\ had the narrowest parameter ranges in most parameters (all except r.max, Fig.\ref{boxFac}).  Datasets describable by this model were generally neutral responses towards all four parameters (Fig.\ref{biptPCA}).  In the analysis for individual parameters, the parameter value ranges overlapping between \pms\ (Fig.\ref{boxFac}).  Hence the differences were not observable although most ``differences" were statistically significant (Fig.\ref{pValDraw}).
 \begin{figure}[H]
 	\centering
 	\includegraphics[width=\linewidth]{../results/Log_outstanding.pdf}
 	\caption{Line plot of datasets restricted to polynomial fits. Dataset details could be found in Table \ref{table:source}}\label{lineOut}
 \end{figure}
	
	\section*{Discussion}
	%% AIC vs BIC <https://www.methodology.psu.edu/resources/AIC-vs-BIC/>
	%% AIC <https://en.wikipedia.org/wiki/Akaike_information_criterion#Comparison_with_BIC>
	%% BIC <https://en.wikipedia.org/wiki/Bayesian_information_criterion>
	%Model fitness to real data and simplistic mathematics were favoured by both AIC\autocite{johnson2004model,akaike1998information,burnhamdr} and BIC\autocite{johnson2004model,turchin2003complex}.  Apart from that, BIC also takes account of sample size effect\autocite{johnson2004model,turchin2003complex}.\\
	%comparisons in different fields\autocite{kuha2004aic,aho2014model,yang2005can,vrieze2012model,wang2006comparison,acquah2010comparison}\\
	
	AIC is considered the most suitable model-selection approach within model and between models in this project.  Unlike BIC, AIC are more accurate with small sample sizes\autocite{acquah2010comparison,kuha2004aic} and sparse data\autocite{kuha2004aic}.  AIC did not assume a ``true model" was under examination\autocite{aho2014model,vrieze2012model,yang2005can}.  Since candidate models were not ``nested model", BIC is not a better choice than AIC\autocite{wang2006comparison}.  Hence the use of only AIC as model-selection criterion should be justified.
	
	Although \fba\ and \fbu\ were observed occupying different parameter space (Fig.\ref{biptPCA}), these differences were not statistical significant (Fig.\ref{barPT}).
	
	\section*{Conclusion}
	Published \pms\ were data-specific, which none of them were found significantly performing better than the others.  Although most of the parameter values are significantly different between models, their ranges are superimposing with one another.  \Pms\ correlate with parameters differently, but the correlations are unobservable through plotting a log-linear logistic growth curve.  There were assumptions embedded within the \pms\ which have limited its ability to describe data without a distinct sigmoid shape.
	
	\section*{Code and Data Availability}
	All \href{https://github.com/ph-u/CMEECourseWork_pmH/tree/master/MiniProject/code}{scripts} and \href{https://github.com/ph-u/CMEECourseWork_pmH/tree/master/MiniProject/data}{data} used for this report were publicity available at GitHub.
	\nocite{*}\printbibliography
	
	\section*{Appendix}
\begin{table}[H]
		\caption{Table showing dataset id details in this project.}\label{table:source}
\end{table}
\begin{small}
	\begin{longtable}{l|llllll}
		id&T$^{o}$C&clade&substrate&replicate&Source&Pop unit\\\hline
		%% insert_num_here
	\end{longtable}
\end{small}
``Source" column publication key:\\
\begin{longtable}{p{.05\linewidth}p{.9\linewidth}}
	%% insert_num_here
	%% insert_num_here
	%% insert_num_here
	%% insert_num_here
	%% insert_num_here
	%% insert_num_here
	%% insert_num_here
	%% insert_num_here
	%% insert_num_here
	%% insert_num_here
\end{longtable}

\end{document}
